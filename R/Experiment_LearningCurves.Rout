
R version 2.15.2 (2012-10-26) -- "Trick or Treat"
Copyright (C) 2012 The R Foundation for Statistical Computing
ISBN 3-900051-07-0
Platform: x86_64-apple-darwin9.8.0/x86_64 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> rm(list=ls())
> source("Requirements.R")
Loading required package: MASS
Loading required package: lpSolve
Loading required package: lattice
Loading required package: reshape
Loading required package: plyr

Attaching package: ‘reshape’

The following object(s) are masked from ‘package:plyr’:

    rename, round_any

The following object(s) are masked from ‘package:class’:

    condense

Loading required package: cluster
Loading required package: foreach

Attaching package: ‘pls’

The following object(s) are masked from ‘package:caret’:

    R2

The following object(s) are masked from ‘package:stats’:

    loadings

Creating a generic function for ‘predict’ from package ‘stats’ in the global environment
Creating a generic function for ‘plot’ from package ‘graphics’ in the global environment
Note: no visible global function definition for 'qplot' 
Note: no visible global function definition for 'geom_abline' 
Creating a generic function for ‘logLik’ from package ‘stats’ in the global environment
Note: no visible binding for global variable 'animation' 
Note: no visible global function definition for 'saveGIF' 
Note: no visible global function definition for 'qplot' 
Note: no visible global function definition for 'as.matrix.csr' 
> 
> library(mlbench)
> library(sampling)
> library(caret)
> library(foreign)
> library(parallel)
> library(plotrix)
> library(ggplot2)
> library(gridExtra)
Loading required package: grid
> 
> ## Settings
> 
> # Output
> outputdir<-"~/"
> current.time<-Sys.time()
> 
> # Measurements
> # classifiers<-c(LeastSquaresClassifierXY,
> #                function(X,y,X_u) {LeastSquaresClassifierXY(X,y,lambda=200)},
> #                ICLeastSquaresClassifierXY,
> #                function(X,y,X_u){ICLeastSquaresClassifierXY(X, y, X_u, lambda1=200,lambda2=200)},
> #                 function(X,y,X_u){ICLeastSquaresClassifierXY(X,y,X_u,lambda1=200,lambda2=0)},
> #                function(X,y,X_u){ICLeastSquaresClassifierXY(X,y,X_u,lambda1=0,lambda2=200)}
> #                )
> # classifiers.names<-c("LS","LS l=1","ICLS","ICLS l=1,1","ICLS l=1,0","ICLS l=0,1")
> 
> classifiers<-c(LogisticRegressionXY,
+                 LeastSquaresClassifierXY,
+                ICLeastSquaresClassifierXY,
+                function(X,y,X_u) {ICLeastSquaresClassifierXY(X,y,rbind(X))},
+                function(X,y,X_u) {ICLeastSquaresClassifierXY(X,y,rbind(X_u,X))})
Note: no visible binding for global variable 'pls' 
Note: no visible binding for global variable 'pls' 
> classifiers.names<-c("LR","LS","ICLS","ICLS2Xl","ICLS2XlXu")
> intercept<-TRUE
> 
> # classifiers<-c(LeastSquaresClassifierXY,
> #                ICLeastSquaresClassifierXY,
> #                NearestMeanClassifierXY,
> #                MCNearestMeanClassifierXY
> # )
> # classifiers.names<-c("LS","ICLS","NM","MCNM")
> 
> # classifiers<-c(LeastSquaresClassifierXY,
> #                function(X,y,X_u) {LogisticRegressionXY(X,y)},
> #                function(X,y,X_u) {EntropyRegularizedLogisticRegressionXY(X,y,X_u)})
> # classifiers.names<-c("LS","LR","ERLR")
> intercept<-TRUE
> 
> # Experiment: Real NMC
> classifiers<-c(function(X,y,X_u) { NearestMeanClassifierXY(X,y,prior=matrix(c(0.5,0.5),2,1)) },
+                function(X,y,X_u) { MCNearestMeanClassifierXY(X,y,X_u, prior=matrix(c(0.5,0.5),2,1))},
+                function(X,y,X_u) { SelfLearningXY(X,y,X_u, method=function(X,y) { NearestMeanClassifierXY(X,y,prior=matrix(c(0.5,0.5),2,1)) })})
> classifiers.names<-c("NM.fixedprior","MCNM.fixedprior","SelfLearning")
> intercept<-FALSE
> 
> # Experiment: Prior free NMC
> classifiers<-c(NearestMeanClassifierXY,
+                MCNearestMeanClassifierXY,
+                function(X,y,X_u) { SelfLearningXY(X, y, X_u, method=NearestMeanClassifierXY)})
Note: no visible binding for global variable 'pls' 
Note: no visible binding for global variable 'pls' 
> classifiers.names<-c("NM","MCNM","SelfLearning")
> intercept<-FALSE
> 
> #Experiment: Least Squares classifier
> classifiers<-c(function(X,y,X_u) {LeastSquaresClassifierXY(X,y) },
+               function(X,y,X_u) {ICLeastSquaresClassifierXY(X,y,X_u) },
+                function(X,y,X_u) { SelfLearningXY(X, y, X_u, method=LeastSquaresClassifierXY)})
> classifiers.names<-c("LS","ICLS","SelfLearning")
> intercept<-TRUE
> 
> # Data
> load("Datasets.RData")
> 
> 
> # For measurements, see below
> measurements.names<-c("Error","Loss Test","Loss Train")
> repeats<-1000
> n_l<-"justenough"
> sizes<-2^(1:12)
> 
> LearningCurve<-function(modelform, data, classifiers, repeats=1000, sampling="empirical") {
+   
+   classname<-all.vars(modelform)[1]
+   #results<-data.frame(Classifier=integer(),Size=integer(),Repeat=integer(),setNames(replicate(length(measurements.names),numeric(0), simplify = F), measurements.names))
+   results<-array(NA, dim=c(repeats, length(sizes), length(classifiers), 3))
+   
+   # Data set to design matrix
+   list2env(SSLDataFrameToMatrices(modelform,data,intercept=intercept),env=environment())
+   
+   # Generate the test sample
+   sample.test<-sample(1:nrow(X), 1000, replace=TRUE)
+   X_test<-X[sample.test,]
+   y_test<-y[sample.test]
+   
+   
+   n_l<-ncol(X)+2
+   for (i in 1:repeats) {
+     print(i) # Print the current repeat
+     
+     sample.labeled<-strata(data,classname,c(1,1),method="srswr")$ID_unit
+     sample.labeled<-c(sample.labeled, sample(1:nrow(data),n_l-2,replace=TRUE))
+     X_l<-X[sample.labeled,]
+     y_l<-y[sample.labeled]
+     
+     sample.unlabeled<-sample(1:nrow(data),max(sizes),replace=TRUE)
+     X_u<-X[sample.unlabeled,]
+     
+ #     if (TRUE) {
+ #       leps<-1.0
+ #       bestl<-1000.0
+ #       for (l in c(10000,1000,500,200,100,10,1,0.1,0.01,0.001,0.0001,0.0000)) {
+ #         #eps<-1-mean(y_l==factor(bootstrap::crossval(X_l, y_l, function(X,y) {LeastSquaresClassifierXY(X,y,lambda=l)}, predict, ngroup=5)$cv.fit))
+ #         eps<-1-mean(y_test==predict(LeastSquaresClassifierXY(X_l,y_l,lambda=l),X_test))
+ #         
+ #         if (eps<leps) {
+ #           bestl<-l
+ #           leps<-eps
+ #         }
+ #       }
+ #       print(leps)
+ #       print(bestl)
+ #     }
+     
+     for (s in 1:length(sizes)) {
+       X_u_s <- X_u[1:sizes[s],]
+       for (c in 1:length(classifiers)) {
+         try({
+         
+         
+ #         classifiers<-c(LeastSquaresClassifierXY,
+ #                        function(X,y,X_u) {LeastSquaresClassifierXY(X,y,lambda=bestl)},
+ #                        ICLeastSquaresClassifierXY,
+ #                        function(X,y,X_u){ICLeastSquaresClassifierXY(X, y, X_u, lambda1=bestl,lambda2=bestl)},
+ #                        function(X,y,X_u){ICLeastSquaresClassifierXY(X,y,X_u,lambda1=bestl,lambda2=0)},
+ #                        function(X,y,X_u){ICLeastSquaresClassifierXY(X,y,X_u,lambda1=0,lambda2=bestl)}
+ #         )
+         
+         trained_classifier<-do.call(classifiers[[c]],list(X_l, y_l, X_u=X_u_s))
+         
+         results[i,s,c,1] <- 1-mean(y_test==predict(trained_classifier,X_test))
+         results[i,s,c,2] <- loss(trained_classifier, X_test, y_test)
+         results[i,s,c,3] <- loss(trained_classifier, X_l, y_l)
+         #results[nrow(results)+1,]<-c(c,sizes[s],i,sapply(measurements,do.call,list(trained_classifier)))
+         #results[i,s,c,1]<-system.time()[3]
+         #results[i,s,c,2:4]<-sapply(measurements,do.call,list(trained_classifier))
+         })
+         }
+     }
+     
+   }
+   # Write Intermediate Results to disk
+   #save.image(file=paste(outputdir,"LearningCurves - ",current.time,".RData",sep=""))
+   return(results)
+ }
> 
> results<-mclapply(names(datasets),function(dname){print(dname); LearningCurve(modelforms[[dname]],datasets[[dname]],classifiers,repeats)})
[1] "Haberman"
[1] "Normals"
[1] 1
[1] 1
[1] 2
[1] 2
[1] 3

Execution halted
