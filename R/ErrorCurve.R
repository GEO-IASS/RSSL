# Error curve class:
# What parameter is changed
# What classifiers
# Names of classifiers
# Plot function
# Different measurements on the data


#' @title Generate Learning Curve for a set of classifiers
#'
#' @description
#' \code{ErrorCurve} runs learning curve experiments for a given dataset and set of classifiers
#'
#' This function generates the supervised learning curve for one or several classifiers. The learning curve is generated by increasing the number of training points used to train the classifier and evaluating performance.
#' 
#' @param X design matrix of the labeled objects
#' @param y vector with labels
#' @param classifiers list; Classifiers to crossvalidate
#' @param with_replacement logical; Whether objects should be drawn with replacement
#' @param sizes integer vector; sizes of the training sets
#' @param n_test integer; number of test objects to be drawn (only when with_replacement==TRUE)
#' @param repeats integer; Number of learning curves to average over
#' @param verbose logical; Controls verbosity of the output
#' 
#' @return A list with the following items:
#' 
#' @export
ErrorCurve<-function(X, y, classifiers, with_replacement=FALSE, sizes=10:10:nrow(X),n_test=1000,repeats=100,verbose=TRUE) {
  
  results<-array(NA, dim=c(repeats, length(sizes), length(classifiers), 4))
  if (is.null(names(classifiers))) {
    classifier_names <- lapply(classifiers, function(c) {as.character(body(c))[[2]]})
  } else {
    classifier_names <- names(classifiers) 
  }
  dimnames(results)<-list(1:repeats,sizes,classifier_names,c("Error", "Loss Test", "Loss Train"))
  sample.test<-sample(1:nrow(X), n_test, replace=TRUE)
  X_test<-X[sample.test,,drop=FALSE]
  y_test<-y[sample.test]
  
  if (verbose) cat("Number of features: ",ncol(X),"\n")
  if (verbose) cat("Number of objects:  ",nrow(X),"\n")
  if (verbose) pb<-txtProgressBar(0,repeats) # Display a text progress bar
  
  for (i in 1:repeats) {
    if (verbose) setTxtProgressBar(pb, i) # Print the current repeat
    
    sample.labeled<- sample_k_per_level(y,1)
    if (with_replacement) {
      sample.labeled<-c(sample.labeled, sample(1:nrow(X),max(sizes)-2,replace=TRUE))
    } else {
      stop("Without replacement not implemented yet.")
    }
    
    X_l<-X[sample.labeled,,drop=FALSE]
    y_l<-y[sample.labeled]
    
    for (s in 1:length(sizes)) {
      X_l_s <- X_l[1:sizes[s],,drop=FALSE]
      y_l_s <- y_l[1:sizes[s]]
      for (c in 1:length(classifiers)) {
        try({
          trained_classifier<-do.call(classifiers[[c]],list(X_l_s, y_l_s, X_u=NULL,y_u=NULL))        
          results[i,s,c,1] <- 1-mean(y_test==predict(trained_classifier,X_test))
          results[i,s,c,2] <- sum(loss(trained_classifier, X_test, y_test))
          results[i,s,c,3] <- sum(loss(trained_classifier, X_l_s, y_l_s))
          results[i,s,c,4] <- sum(loss(trained_classifier, X_l_s, y_l_s)) # Area under ROC curve
        })
      }
    }
  }
  if (verbose) cat("\n")
  return(list(call="Not known",results=results,n_test=n_test,independent="Number of training objects"))
}

#' @title Generate Semi-Supervised Learning error curve
#' 
#' @description Generate Semi-Supervised Learning error curve
#' 
#' @param X design matrix
#' @param y vector of labels
#' @param classifiers list; Classifiers to crossvalidate
#' @param n_l Number of labeled objects to be used in the experiments
#' @param with_replacement Indicated whether the subsampling is done with replacement or not (default: FALSE)
#' @param sizes vector with number of unlabeled objects for which to evaluate performance
#' @param n_test Number of test points if with_replacement is TRUE
#' @param repeats Number of learning curves to draw
#' @param n_min Minimum number of labeled objects per class in
#' @param verbose Print progressbar during execution (default: FALSE)
#' @param dataset_name character; Name of the dataset
#' 
#' @return ErrorCurve object
#' 
#' 
#' @export
ErrorCurveSSL<-function(X, y, classifiers, n_l, with_replacement=FALSE, sizes=2^(1:8), n_test=1000,repeats=100, verbose=FALSE,n_min=1,dataset_name=NULL) {
  
  #TODO sanity checks: X and y should be the same length, should be matrices?
  if (!is.factor(y)) { stop("Labels are not a factor.") }
  K <- length(levels(y))
  
  results<-array(NA, dim=c(repeats, length(sizes), length(classifiers), 4))
  if (is.null(names(classifiers))) {
    classifier_names <- lapply(classifiers, function(c) {as.character(body(c))[[2]]})
  } else {
    classifier_names <- names(classifiers) 
  }
  dimnames(results)<-list(1:repeats,sizes,classifier_names,c("Error", "Avg. Loss Test", "Avg. Loss Train","Avg. Loss Lab+Unlab"))
  
  if (verbose) cat("Number of features: ", ncol(X),"\n")
  if (verbose) cat("Number of objects:  ", nrow(X),"\n")
  if (verbose) pb <- txtProgressBar(0,repeats) # Display a text progress bar
  
  for (i in 1:repeats) {
    if (verbose) setTxtProgressBar(pb, i) # Print the current repeat
    
    sample.labeled <- sample_k_per_level(y,n_min)
    sample.labeled <- c(sample.labeled, sample((1:nrow(X))[-sample.labeled],n_l-(K*n_min),replace=FALSE))

    X_l <- X[sample.labeled,,drop=FALSE]
    y_l <- y[sample.labeled]
    
    if (!with_replacement) {
      sample.unlabeled <- sample((1:nrow(X))[-sample.labeled])
    } else {
      sample.unlabeled <- sample(1:nrow(X),max(sizes)+n_test,replace=TRUE)
    }
    X_u <- X[sample.unlabeled,,drop=FALSE]
    y_u <- y[sample.unlabeled]
    
    for (s in 1:length(sizes)) {
      if (sizes[s]>nrow(X_u)) {break}
      
      X_u_s <- X_u[1:sizes[s],,drop=FALSE]
      y_u_s <- y_u[1:sizes[s]]
      if (!with_replacement) {
        X_test <- X_u[-(1:sizes[s]),,drop=FALSE]
        y_test <- y_u[-(1:sizes[s])]
      } else {
        X_test <- X_u[-(1:max(sizes)),,drop=FALSE]
        y_test <- y_u[-(1:max(sizes))]
      }
      
      prX_l <- X_l
      prX_u_s <- X_u_s
      prX_test <- X_test
      #For all preprocessing
#             pca<-prcomp(X_l)
#             # print(rankMatrix(X_l))
#             prX_l<-(predict(pca, X_l))[,1:(rankMatrix(X_l)-1)]
#             prX_u_s<-(predict(pca, X_u_s))[,1:(rankMatrix(X_l)-1)]
#             prX_test<-(predict(pca, X_test))[,1:(rankMatrix(X_l)-1)]
      # cat(nrow(X_test),nrow(X_u_s),nrow(X_l),"\n")
      for (c in 1:length(classifiers)) {
        

        trained_classifier<-do.call(classifiers[[c]],list(X=prX_l, y=y_l, X_u=prX_u_s, y_u=y_u_s))
        
        results[i,s,c,1] <- 1-mean(y_test==predict(trained_classifier,prX_test))
        results[i,s,c,2] <- mean(loss(trained_classifier, prX_test, y_test))
        results[i,s,c,3] <- mean(loss(trained_classifier, prX_l, y_l))
        results[i,s,c,4] <- mean(loss(trained_classifier, rbind(prX_l,prX_u_s), unlist(list(y_l,y_u_s))))
      }
    }
  }
  if (verbose) cat("\n")
  object<-list(call="Not known"
               ,n_l=n_l,
               results=results,
               n_test=n_test,
               independent="Number of unlabeled objects")
  class(object)<-"ErrorCurve"
  return(object)
}

ErrorCurveTransductive <- function(X, y, classifiers, n_l, with_replacement=FALSE, sizes=2^(1:8),repeats=100, verbose=FALSE,n_min=1,dataset_name=NULL) {
  
  #TODO sanity checks: X and y should be the same length, should be matrices?
  if (!is.factor(y)) { stop("Labels are not a factor.") }
  K <- length(levels(y))
  
  results<-array(NA, dim=c(repeats, length(sizes), length(classifiers), 3))
  if (is.null(names(classifiers))) {
    classifier_names <- lapply(classifiers, function(c) {as.character(body(c))[[2]]})
  } else {
    classifier_names <- names(classifiers) 
  }
  dimnames(results)<-list(1:repeats,sizes,classifier_names,c("Error Unlab", "Avg. Loss Unlab","Avg. Loss Lab+Unlab"))
  
  if (verbose) cat("Number of features: ", ncol(X),"\n")
  if (verbose) cat("Number of objects:  ", nrow(X),"\n")
  if (verbose) pb <- txtProgressBar(0,repeats) # Display a text progress bar
  
  for (i in 1:repeats) {
    if (verbose) setTxtProgressBar(pb, i) # Print the current repeat
    
    sample.labeled <- sample_k_per_level(y,n_min)
    sample.labeled <- c(sample.labeled, sample((1:nrow(X))[-sample.labeled],n_l-(K*n_min),replace=FALSE))
    
    X_l <- X[sample.labeled,,drop=FALSE]
    y_l <- y[sample.labeled]
    
    if (!with_replacement) {
      sample.unlabeled <- sample((1:nrow(X))[-sample.labeled])
    } else {
      sample.unlabeled <- sample(1:nrow(X),max(sizes),replace=TRUE)
    }
    X_u <- X[sample.unlabeled,,drop=FALSE]
    y_u <- y[sample.unlabeled]
    
    for (s in 1:length(sizes)) {
      if (sizes[s]>nrow(X_u)) {break}
      
      X_u_s <- X_u[1:sizes[s],,drop=FALSE]
      y_u_s <- y_u[1:sizes[s]]
      
      prX_l <- X_l
      prX_u_s <- X_u_s
      #For all preprocessing
      #             pca<-prcomp(X_l)
      #             # print(rankMatrix(X_l))
      #             prX_l<-(predict(pca, X_l))[,1:(rankMatrix(X_l)-1)]
      #             prX_u_s<-(predict(pca, X_u_s))[,1:(rankMatrix(X_l)-1)]
      #             prX_test<-(predict(pca, X_test))[,1:(rankMatrix(X_l)-1)]
      # cat(nrow(X_test),nrow(X_u_s),nrow(X_l),"\n")
      for (c in 1:length(classifiers)) {
        
        
        trained_classifier<-do.call(classifiers[[c]],list(X=prX_l, y=y_l, X_u=prX_u_s, y_u=y_u_s))
        
        results[i,s,c,1] <- 1-mean(y_u_s==predict(trained_classifier,prX_u_s))
        results[i,s,c,2] <- 1-mean(y_u_s==trained_classifier@unlab_predictions)
        results[i,s,c,3] <- 0 #mean(loss(trained_classifier, prX_l, y_l))
      }
    }
  }
  if (verbose) cat("\n")
  object<-list(call="Not known"
               ,n_l=n_l,
               results=results,
               independent="Number of unlabeled objects")
  class(object)<-"ErrorCurve"
  return(object)
}

#' Concatenate ErrorCurve objects
c.ErrorCurve <- function(x,...,recursive=FALSE) {}

#' @title Plot ErrorCurve object
#' 
#' @description Plot ErrorCurve object generated by the ErrorCurve or ErrorCurveSSL functions
#' 
#' @details If multiple ErrorCurve objects are supplied, the metadata from the first object will be used to draw the curves. Dataset names should be distinct
#' @param x ErrorCurve object OR a list of errorcurve objects TODO: fix for list
#' @param measurement The number of the measurement that should be plotted
#' @param legendsetting ggplot2 option Where the legend should be plotted (default: right)
#' @param dataset_names A vector with dataset names corresponding to the ErrorCurve objects
#' @param classifier_names A vector with (usually shortened) classifier names
#' @param ncol integer; Number of columns in the plot
#' @param ... Unused
#' 
#' @return a ggplot2 object constaining the figure
#' 
#' @export
plotErrorCurve<-function(x, measurement=1,legendsetting="right",dataset_names=NULL,classifier_names=NULL,ncol=2,...) {
  require(data.table)
  require(reshape)
  require(ggplot2)
  
  data<-x
  
  # Check for input object
  if (class(data)=="ErrorCurve") { 
    data <- list(data)
  } else if (!(is.list(data) & all(lapply(data,class)=="ErrorCurve"))) {
    stop("Input object should be ErrorCurve of list of ErrorCurve objects.")
  }
  
  # Extract metadata from the first experiment
  m<-measurement
  x_label <- data[[1]]$independent
  y_label <- dimnames(data[[1]]$results)[[4]][m]

  # Generate the dataset for plotting
  dataframes.merged<-list()
  for (i in 1:length(data)) {
    data_i<-data[[i]]
    
    results<-data_i$results[,,,,drop=FALSE]
    
    classifiers.names<-dimnames(data_i$results)[[3]]
    sizes<-dimnames(data_i$results)[[2]]
    
    results.mean<-apply(results[,,,,drop=FALSE],c(2,3,4),mean,na.rm=T)[,,m]
    results.stderror<-apply(results[,,,,drop=FALSE],c(2,3,4),stderror)[,,m]
    
    results.merged<-merge(reshape::melt(results.mean),reshape::melt(results.stderror),by=c("X1","X2"))
    names(results.merged)<-c("Size","Classifier","Mean","Std.Error")
    dataset_name <- ifelse(is.null(dataset_names),i,dataset_names[[i]])
    
    dataframes.merged[[i]]<-data.frame(results.merged,Dataset=dataset_name,Measurement=1)
  }
  plotdata<-rbindlist(dataframes.merged)
  
  # Reorder and relabel the classifier names
  if (!is.null(classifier_names)) {
    plotdata$Classifier<-factor(plotdata$Classifier,levels=classifiers.names,labels=classifier_names)
  } else {
    plotdata$Classifier<-factor(plotdata$Classifier,levels=classifiers.names)
  }

  h <- ggplot(plotdata, aes_string(x="factor(Size)",y="Mean",group="Classifier",color="Classifier")) +
    facet_wrap(~ Dataset, ncol = ncol,scales="free_y") +
    geom_line(aes_string(y="Mean",group="Classifier",color="Classifier")) +
    geom_point(aes_string(y="Mean",group="Classifier",color="Classifier")) +
    geom_ribbon(aes_string(ymin="Mean-Std.Error", ymax="Mean+Std.Error",group="Classifier",color="Classifier",fill="Classifier"),colour=NA,alpha=0.5) +
    scale_x_discrete(breaks = sizes, labels=sizes) +
    xlab(x_label) +
    ylab(y_label) +
    theme_bw() +
#     theme( 
  #     plot.background = element_blank() ,
  #     panel.grid.major = element_blank() ,
  #     panel.grid.minor = element_blank() ,
  #     panel.border = element_blank() ,
  #     panel.background = element_blank(),
  #     legend.position = legendsetting,
  #     legend.text = element_text(size=12),
  #     axis.ticks = element_line(colour = "black"),
  #     axis.title.x = element_text(size = 10, vjust = 0.5),
  #     axis.title.y = element_text(size = 10, angle = 90, vjust = 0.5),
  #     axis.text.x = element_text(size = 7, lineheight = 0.9, colour = "black", vjust = 1),
  #     axis.text.y = element_text(size = 7, lineheight = 0.9, colour = "black", hjust = 1)
  #   ) +
    theme(axis.line = element_line(color = 'black'), strip.background=element_rect(fill="white",colour="white"), legend.position=legendsetting, panel.border=element_blank(), strip.text=element_text(size=14), 
          axis.line=element_line())
  return(h)
}

#' Plot of difference between supervised and semi-supervised errors
#' 
#' @param data ErrorCurve object
#' @param measurement Index of the measurement to be plotted
#' @param legendsetting Where the legend should be placed
#' @param dataset_names Names of the datasets in the ErrorCurve object
#' @param classifier_names Names of the classifiers in the ErrorCurve object
#' 
#' @export
DifferencePlot<-function(data,measurement=1,legendsetting="right",dataset_names=NULL,classifier_names=NULL) {
  require(data.table)
  require(reshape)
  require(ggplot2)
  
  # Check for input object
  if (class(data)=="ErrorCurve") { 
    data <- list(data)
  } else if (!(is.list(data) & all(lapply(data,class)=="ErrorCurve"))) {
    stop("Input object should be ErrorCurve of list of ErrorCurve objects.")
  }
  
  # Extract metadata from the first experiment
  m<-measurement
  x_label <- data[[1]]$independent
  y_label <- dimnames(data[[1]]$results)[[4]][m]
  
  # Generate the dataset for plotting
  dataframes.merged<-list()
  for (i in 1:length(data)) {
    data_i<-data[[i]]
    
    results<-data_i$results[,,,,drop=FALSE]
    
    classifiers.names<-dimnames(data_i$results)[[3]]
    sizes<-dimnames(data_i$results)[[2]]
    
    res_dif1 <- results[,1,1,m]-results[,1,2,m]
    res_dif2 <- results[,1,1,m]-results[,1,3,m]
#     res_dif3 <- results[,1,4,m]-results[,1,5,m]
    dataset_name <- ifelse(is.null(dataset_names),i,dataset_names[[i]])
    
#     dataframes.merged[[i]]<-fill(data.frame(res_dif=res_dif1,Dataset=dataset_name,Classifier="Projection"), data.frame(res_dif=res_dif2,Dataset=dataset_name,Classifier="Self Learning"), data.frame(res_dif=res_dif3,Dataset=dataset_name,Classifier="TSVM"))
dataframes.merged[[i]]<-rbindlist(data.frame(res_dif=res_dif1,Dataset=dataset_name,Classifier="Projection"), data.frame(res_dif=res_dif2,Dataset=dataset_name,Classifier="Self Learning"))
  }
  plotdata<-rbindlist(dataframes.merged)
  # Reorder and relabel the classifier names
#   if (!is.null(classifier_names)) {
#     plotdata$Classifier<-factor(plotdata$Classifier,levels=classifiers.names,labels=classifier_names)
#   } else {
#     plotdata$Classifier<-factor(plotdata$Classifier,levels=classifiers.names)
#   }
  
  h <- ggplot(plotdata, aes_string(x='Dataset',y='res_dif',color='Classifier',group='Classifier')) + geom_point(size=3) + facet_wrap(~ Classifier, ncol = 3, scales="free_y") + theme_bw() + theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position="none",axis.title.x = element_blank()) + ylab("Loss difference") + scale_y_continuous(limits = c(-1,1 )) + geom_hline(aes(yintercept=0))
  return(h)
}

#' @title Plot ErrorCurve object in a different way
#' 
#' @description TODO: check how this is a different function
#' 
#' @details If multiple ErrorCurve objects are supplied, the metadata from the first object will be used to draw the curves. Dataset names should be distinct
#' @param x ErrorCurve object OR a list of errorcurve objects
#' @param measurement The number of the measurement that should be plotted
#' @param legendsetting ggplot2 option Where the legend should be plotted (default: right)
#' @param dataset_names A vector with dataset names corresponding to the ErrorCurve objects
#' @param classifier_names A vector with (usually shortened) classifier names
#' @param ncol integer; Number of columns in the plot
#' @param ... Unused
#' 
#' @return a ggplot2 object constaining the figure
#' 
#' @export
plotErrorCurve2<-function(x,measurement=1,legendsetting="right",dataset_names=NULL,classifier_names=NULL,ncol=2,...) {
  require(data.table)
  require(reshape)
  require(ggplot2)
  
  data <- x
  # Check for input object
  if (class(data)=="ErrorCurve") { 
    data <- list(data)
  } else if (!(is.list(data) & all(lapply(data,class)=="ErrorCurve"))) {
    stop("Input object should be ErrorCurve of list of ErrorCurve objects.")
  }
  
  # Extract metadata from the first experiment
  m<-measurement
  x_label <- data[[1]]$independent
  y_label <- dimnames(data[[1]]$results)[[4]][m]
  
  # Generate the dataset for plotting
  dataframes.merged<-list()
  for (i in 1:length(data)) {
    data_i<-data[[i]]
    
    results<-data_i$results[,,,,drop=FALSE]
    
    classifiers.names<-dimnames(data_i$results)[[3]]
    sizes<-dimnames(data_i$results)[[2]]
    
    results.mean1<-apply(results[,,,,drop=FALSE],c(2,3,4),mean,na.rm=T)[,,1]
    results.stderror1<-apply(results[,,,,drop=FALSE],c(2,3,4),stderror)[,,1]
    
    results.merged1<-merge(reshape::melt(results.mean1),reshape::melt(results.stderror1),by=c("X1","X2"))
    names(results.merged1)<-c("Size","Classifier","Mean","Std.Error")
    
    results.mean2<-apply(results[,,,,drop=FALSE],c(2,3,4),mean,na.rm=T)[,,2]
    results.stderror2<-apply(results[,,,,drop=FALSE],c(2,3,4),stderror)[,,2]
    
    results.merged2<-merge(reshape::melt(results.mean2),reshape::melt(results.stderror2),by=c("X1","X2"))
    names(results.merged2)<-c("Size","Classifier","Mean","Std.Error")
    dataset_name <- ifelse(is.null(dataset_names),i,dataset_names[[i]])
    
    dataframes.merged[[i]]<-data.frame(results.merged1,Dataset=dataset_name,Measurement="Error")
    dataframes.merged[[i+length(data)]]<-data.frame(results.merged2,Dataset=dataset_name,Measurement=dimnames(data[[1]]$results)[[4]][2])
  }
  plotdata<-rbindlist(dataframes.merged)
  
  # Reorder and relabel the classifier names
  if (!is.null(classifier_names)) {
    plotdata$Classifier<-factor(plotdata$Classifier,levels=classifiers.names,labels=classifier_names)
  } else {
    plotdata$Classifier<-factor(plotdata$Classifier,levels=classifiers.names)
  }
  
  h <- ggplot(plotdata, aes_string(x="factor(Size)",y="Mean",group="Classifier",color="Classifier")) +
    facet_wrap(Dataset ~ Measurement, ncol=ncol, scales="free") +
    geom_line(aes_string(y="Mean",group="Classifier",color="Classifier")) +
    geom_point(aes_string(y="Mean",group="Classifier",color="Classifier")) +
    geom_ribbon(aes_string(ymin="Mean-Std.Error", ymax="Mean+Std.Error",group="Classifier",color="Classifier",fill="Classifier"),colour=NA,alpha=0.5) +
    scale_x_discrete(breaks = sizes, labels=sizes) +
    xlab(x_label) +
    ylab("")  +
    theme_bw()+
    theme(axis.text = element_text(size=7))
    #     theme( 
    #     plot.background = element_blank() ,
    #     panel.grid.major = element_blank() ,
    #     panel.grid.minor = element_blank() ,
    #     panel.border = element_blank() ,
    #     panel.background = element_blank(),
    #     legend.position = legendsetting,
    #     legend.text = element_text(size=12),
    #     axis.ticks = element_line(colour = "black"),
    #     axis.title.x = element_text(size = 10, vjust = 0.5),
    #     axis.title.y = element_text(size = 10, angle = 90, vjust = 0.5),
  #     axis.text.x = element_text(size = 7, lineheight = 0.9, colour = "black", vjust = 1),
  #     axis.text.y = element_text(size = 7, lineheight = 0.9, colour = "black", hjust = 1)
  #   ) +
  
  return(h)
}

#' Sample k indices per levels from a factor 
#' @param y factor; factor with levels
#' @param k integer; number of indices to sample per level
#' @return vector with indices for sample
#' @export
sample_k_per_level <- function(y,k) {
  stopifnot(is.factor(y))
  stopifnot(k>0)
  
  all_idx <- 1:length(y)
  sample_idx <- c()
  for (i in levels(y)) {
    sample_idx <- c(sample_idx,sample(all_idx[y==i],k))
  }
  return(sample_idx)
}

#' Generate Learning Curve for losses at different fractions of objects labeled
#' @export
learningcurve_fraction_labeled <- function(X,y,classifiers,fracs=seq(0.1,0.9,by=0.1), with_replacement=FALSE,repeats=10,verbose=FALSE,n_min=1, test_fraction=NULL) {
  if (!is.factor(y)) { stop("Labels are not a factor.") }
  K <- length(levels(y))
  
  results<-array(NA, dim=c(repeats, length(fracs), length(classifiers), 4))
  if (is.null(names(classifiers))) {
    classifier_names <- lapply(classifiers, function(c) {as.character(body(c))[[2]]})
  } else {
    classifier_names <- names(classifiers) 
  }
  dimnames(results)<-list(1:repeats,fracs,classifier_names,c("Error", "Avg. Loss Test", "Avg. Loss Train","Avg. Loss Lab+Unlab"))
  
  n <- nrow(X)
  
  if (verbose) cat("Number of features: ", ncol(X),"\n")
  if (verbose) cat("Number of objects:  ", nrow(X),"\n")
  if (verbose) pb <- txtProgressBar(0,repeats) # Display a text progress bar
  
  for (i in 1:repeats) {
    if (verbose) setTxtProgressBar(pb, i) # Print the current repeat
    sample.guaranteed <- sample_k_per_level(y,n_min)
    if (!is.null(test_fraction)) { 
      idx_test <- sample((1:nrow(X))[-sample.guaranteed], size=ceiling(n*test_fraction))
      sampleorder <- c(sample.guaranteed,sample((1:nrow(X))[-c(sample.guaranteed,idx_test)]))
    } else {
      sampleorder <- c(sample.guaranteed,sample((1:nrow(X))[-c(sample.guaranteed)]))
    }
    
    for (s in 1:length(fracs)) {
      idx_lab <- sampleorder[1:ceiling(length(sampleorder)*fracs[s])]
      
      X_l <- X[idx_lab,,drop=FALSE]
      y_l <- y[idx_lab]
      if (!is.null(test_fraction)) {
        #Separate test set
        X_u <- X[-c(idx_lab,idx_test),,drop=FALSE]
        y_u <- y[-c(idx_lab,idx_test)]
        
        X_test <- X[idx_test,,drop=FALSE]
        y_test <- y[idx_test]
      } else {
        #test on unlabeled data
        X_u <- X[-c(idx_lab),,drop=FALSE]
        y_u <- y[-c(idx_lab)]
        
        X_test <- X_u
        y_test <- y_u
      }
      
      for (c in 1:length(classifiers)) {
        try({
        trained_classifier<-do.call(classifiers[[c]],list(X=X_l, y=y_l, X_u=X_u, y_u=y_u))
        
        results[i,s,c,1] <- 1-mean(y_test==predict(trained_classifier,X_test))
        results[i,s,c,2] <- mean(loss(trained_classifier, X_test, y_test))
        results[i,s,c,3] <- mean(loss(trained_classifier, X_l, y_l))
        results[i,s,c,4] <- mean(loss(trained_classifier, rbind(X_l,X_u), unlist(list(y_l,y_u))))
        })
      }
    }
  }
  if (verbose) cat("\n")
  object<-list(call="Not known", results=results, independent="Fraction of labeled objects")
  class(object)<-"ErrorCurve"
  return(object)
}
